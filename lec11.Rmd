---
title: "Chapter 9 coding"
author: "DJM"
date: "8 March 2018"
output:
  slidy_presentation:
    css: http://mypage.iu.edu/~dajmcdon/teaching/djmRslidy.css
    font_adjustment: 0
  pdf_document: default
---


\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Var}[1]{\mathbb{V}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\argmin}[1]{\underset{#1}{\textrm{argmin}}}
\newcommand{\tr}[1]{\mbox{tr}(#1)}

## Mid-semester evaluation

```{r mid-sem-data,echo=FALSE}
hrs = factor(c(2,4,2,2,4,2,4,3,2,3,2,3,2), levels=1:5,labels = c('0-3','3-6','6-10','10-15','15+'))
knitr::kable(table(hrs))
```


Comments:

* More math, less coding (3)
* More coding, less math (6)
* Text helps (3)
* Text doesn't help (1)
* Prefer group HW (5)
* Detest group HW (3)

Things to do:

* More worked examples (like hedge fund, California)
* Enforce HW participation, too much HW free-loading.
* Make you read things __before__ class. (But people don't care much for RRs.)
* A request for in-class coding.


## GAMs

```{r setup, echo=FALSE, results='hide',message=FALSE}
# Need the knitr package to set chunk options
library(knitr)
# Set knitr options for knitting code into the report:
# - Don't print out code (echo)
# - Save results so that code blocks aren't re-run unless code changes (cache),
# _or_ a relevant earlier code block changed (autodep), but don't re-run if the
# only thing that changed was the comments (cache.comments)
# - Don't clutter R output with messages or warnings (message, warning)
  # This _will_ leave error messages showing up in the knitted report
opts_chunk$set(message=FALSE, warning=FALSE, fig.align='center',fig.width=10,
               fig.height=4,cache=TRUE, autodep=TRUE)
options(show.signif.stars=FALSE)
library(tidyverse)
theme_set(theme_minimal(base_family="Times"))
green = '#00AF64'
blue = '#0B61A4'
red = '#FF4900'
orange = '#FF9200'
```

* Here we introduce the concept of GAMs ( __G__ eneralized __A__ dditive __M__ odels)

* The basic idea is to imagine that the response is the sum of some functions of the predictors:
\[
\Expect{Y_i \given X_i=x_i} = \alpha + f_1(x_{i1})+\cdots+f_p(x_{ip}).
\]

* Note that OLS __is__ a GAM (take $f_j(x_{ij})=\beta_j x_{ij}$):
\[
\Expect{Y_i \given X_i=x_i} = \alpha + \beta_1 x_{i1}+\cdots+\beta_p x_{ip}.
\]

* The algorithm for fitting these things is called "backfitting":
    
    1. Center $Y$ and $X$.
    2. Hold $f_k$ for all $k\neq j$ fixed, and regress $f_j$ on the partial residuals using your favorite smoother.
    3. Repeat for $1\leq j\leq p$.
    4. Repeat steps 2 and 3 until the estimated functions "stop moving" (iterate)
    5. Return the results.
    
## Results

* We will code it today.

* There are two `R` packages that do this for us. I find `mgcv` easier.


## The code

```{r}
backfitlm <- function(y, X, max.iter=100, small.enough=1e-4, track=FALSE){
  # This function performs linear regression by "backfitting" 
  # Inputs: y - the response
  #         X - the design matrix
  #         max.iter - the maximum number of loops through the predictors (default to 100)
  #         small.enough - if the mse changes by less than this, terminate (default to 1e-6)
  X = as.matrix(X) 
  p = ncol(X) # how many covariates?
  n = nrow(X) # how many observations
  betas = double(p) # create a vector for our estimated coefficients (all zeros now)
  preds = matrix(0, n, p) # a matrix to hold the partial predictions of each covariate
  pres = matrix(y, n, p) # partial residuals begin as y (since we are predicting with 0)
  iter = 0 # initialize our iteration
  mse = mean(y^2) # initialize the MSE to SSTo
  conv = FALSE # initialize the convergence check to FALSE
  while(!conv && (iter < max.iter)){ # enter the loop, check conditions (note && not &)
    iter = iter + 1 # update the iteration count
    for(j in 1:p){ # loop over all predictors
      pres[,j] = y - rowSums(preds[,-j]) # partial residuals (ignoring current predictor)
      ## same as X[,-j] %*% betas[-j], same as apply(preds[,-j],1,sum)
      mod = lm(pres[,j] ~ X[,j]-1) # regress current predictor on partial residuals, no intercept, if gam, remove
      ## mod = gam(pres[,j]~X[,j]) # if we want a gam instead
      betas[j] = coefficients(mod) # get out the single coefficient, if gam, remove this line
      preds[,j] = fitted(mod) # update the predictions from this column
    }
    msenew = sqrt(mean((y - rowSums(preds)))^2) # get the updated MSE after a pass
    conv = (abs(mse-msenew)<small.enough) # check how different our MSE was from previous
    mse = msenew # save the new MSE
    if(track) cat(iter/max.iter, " mse = ", mse, "\n")
  }
  return(list(bhat=betas, pres=pres, preds = preds, mse=mse)) # return our coefficients
}
```


## Testing...

* Generate a design matrix $X$ with 100 observations and $p=10$ covariates and a response variable $y$ using a linear model. 

* Test the (now complete) `backfitlm` on this data and compare the results to `lm`. Should there be an intercept in either version?

```{r}
set.seed(03-08-2017)
n = 100
p = 10
X = matrix(runif(n*p,-1,1), n)
b = 10:1 # true betas
y = X %*% b + rnorm(n, sd=.5)
bhat.lm = coef(lm(y~X-1)) # no intercept
bhat.bf = backfitlm(y,X)$bhat # also no intercept
round(rbind(bhat.lm,bhat.bf),3)
```

Notice that the estimated coefficients are exactly the same. I didn't generate data with an intercept, so I didn't let `lm` estimate one. You could have though. You just need to include a column of ones in the `X` matrix.



